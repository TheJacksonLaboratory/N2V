{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embedding using Binary SkipGram\n",
    "The SkipGram model predicts wethever two values extracted from the available vocabulary are in the same context window or not.\n",
    "\n",
    "In our implementatation, as for the CBOW model, since the batches walks are lazily generated, the memory requirements are minimal and the method can scale to very big graphs. It can also run on graphs like Monarch (150M edges and 50M nodes) provided that you use a GPU ([or better still a TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#console)) that is able to fit an embedding model that big, but that is just related to the shear number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto # Import needed to avoid TensorFlow warnings and general useless infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the graphs\n",
    "We load the ppi graph from the repository as a weighted undirected graph.\n",
    "\n",
    "To load the graph we are using the sister package of Embiggen called [Ensmallen](https://github.com/LucaCappelletti94/ensmallen_graph). Ensmallen is a Rust library with python bindings to handle processing of graph files and preprocessing of data for quickly training embedding models.\n",
    "\n",
    "It also supports numerous utilities that can be helpful when dealing with graphs, such as ways to compute holdouts of different kinds on the graph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "graph = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"../data/ppi/edges.tsv\",\n",
    "    sources_column=\"subject\",\n",
    "    destinations_column=\"object\",\n",
    "    directed=False,\n",
    "    weights_column=\"weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first thing, we print a short report showing all the avalable graph details, including the number of edges, nodes, trap nodes and both the connected components and the strongly connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'connected_components_number': '181',\n",
       " 'edges_number': '588748',\n",
       " 'unique_node_types_number': '0',\n",
       " 'traps_rate': '0',\n",
       " 'strongly_connected_components_number': '181',\n",
       " 'degrees_mean': '34.25941227814955',\n",
       " 'selfloops_percentage': '0',\n",
       " 'singleton_nodes': '0',\n",
       " 'unique_edge_types_number': '0',\n",
       " 'bidirectional_percentage': '1',\n",
       " 'density': '0.001993564869255138',\n",
       " 'is_directed': 'false',\n",
       " 'degrees_median': '11',\n",
       " 'degrees_mode': '1',\n",
       " 'nodes_number': '17185'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training and validation partitions\n",
    "To execute the embedding of the graph with the goal of running Link Prediction on the obtained vectors (after having obtained the edges embeddings from the nodes embeddings), we need to create two partitions of the graph. One containing the training set edges, which is left connected (if the initial graph was connected) or at least with the same number of connected components are the initial graph, and one containing the validation set edges, which may not be connected.\n",
    "\n",
    "The requirement for connectivity is stated in the seminal work from [Leskovec et al.](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) but we have no proofs, currently, that actually changes performance of the embedding.\n",
    "\n",
    "#### Limitation in using connected training set\n",
    "Creating holdouts in this fashion allows us to estimate the real-world model performance only for cases when the new edges that we intend to predict are part of the existing graph components and do not ever connect separated components of the graph.\n",
    "\n",
    "### Why not simply using the complete graph in the embedding process\n",
    "If we were to use the complete graph during the embedding process, we would end up encoding the answers for the Link Prediction problem into the embedding for both the training and validation partitions, hence adding a positive bias that would inflate the performance of the models.\n",
    "\n",
    "**NB: the entire graph should be used for embeddings when the task will be a node-label prediction task, when no nodes labels are taken in consideration in any shape of form during the random walks.**\n",
    "\n",
    "### The inevitable presence of false negatives in the embedding training process\n",
    "\n",
    "In the training set of the model there will be inevitably some false negatives. These originates from the sampling of random edges as negatives.\n",
    "\n",
    "In Binary SkipGram, meaning the models that receive two words and tries to predict if the connection exists or not, this sampling is needed to produce negative examples so that the network don't overfit returning always true.\n",
    "\n",
    "In Categorical-Models, such as CBOW and non-binary SkipGram, meaning the one which either receive the context and tries to predict the central word, or receives the word and tries to predict the context, the sample is only present if they use the [Noise Contrastive Estimation (NCE)](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) loss which, instead of computing the softmax and the categorical cross entropy, samples a random number of categories per batch and applies, to the choosen outputs, a sigmoid activation function, hence computing the logits, and then computes the binary cross-entropy for each of them. Finally the losses are summed to obtain the final loss.\n",
    "\n",
    "In this case, the problem of false negatives araise from the fact that during the computation of the NCE loss, in order to have negative examples, the function samples random words to use as wrong predictions for the given context. This of course could create false negatives if the choosen random word appears in another batch with the same context, and can be easily shown that this leads to sampling potentialy false negative edges.\n",
    "\n",
    "The only method to avoid these is to do the **embedding on the whole graph** which create a positive bias for the link-prediction.\n",
    "This bias is formed because the embedding is encoding the data by learning which nodes are close and **thus is encoding the link prediction of the validation set and invalidating the link-prediction performances**.\n",
    "\n",
    "Therefore, the only way is to accept the false negatives since in a real world application the link prediction is used as an oracle to explore new possible edges. \n",
    "\n",
    "#### Additional false positives relative to collisions with known training edges\n",
    "For the same sampling issue, there will be also collisions with the training edges. This should be a lesser problem though, as the model will be more likely to see the positive label for any given training edge that the collision, though to the strong imbalance between the number of available training edges and the complete number of edges.\n",
    "\n",
    "This imbalance, in PPI, is around 1 positive edge to 1000 negative edges.\n",
    "\n",
    "### Early stopping criterion for the embedding process\n",
    "For the early stopping criterion used in the embedding process, during the model selection (the procedure where we select the optimal $p$, $q$ and embedding size parameters, amongs others) we can use the validation partition to stop the training early (hence early stopping) when a metric representing of the quality of the embedding stops improving.\n",
    "\n",
    "Since the validation set is not connected we cannot diretly use the validation set for th early stopping, as it would not allow us to create random walks that represent the graph structure.\n",
    "\n",
    "For this reason we need to use the entire training+validation set for the early stopping. This will allow us to compute random walks that should be representative of the graph structure.\n",
    "\n",
    "The validation edges are never fed into the training model as positives, but are only used in the context of the early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the graph into the two edge partitions\n",
    "As described above, we split the graphs into two partitions:\n",
    "\n",
    "- The training component, with the same number of components as the initial graph\n",
    "- The validation component, that may have more components than the initial graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = graph.connected_holdout(42, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are check that are not necessary, but are offered as sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert graph > training\n",
    "assert graph > validation\n",
    "assert (training + validation).contains(graph)\n",
    "assert graph.contains(training + validation)\n",
    "assert not training.overlaps(validation)\n",
    "assert not validation.overlaps(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considered parameters\n",
    "We are going to use the following parameters:\n",
    "\n",
    "- **Walk lengths:** $100$ nodes.\n",
    "- **Batch size:** $2^{7} = 128$ walks per batch.\n",
    "- **Walk iterations:** $20$ iterations on the graph.\n",
    "- **Window size:** $4$ nodes, meaning $4$ on the left and $4$ on the right of the center nodes. Consider that the first *window_size* values on the left and the right of the walks will be trimmed.\n",
    "- **Return weight, inverse of $p$:** $1.0$.\n",
    "- **Explore weight, inverse of $q$:** $1.0$.\n",
    "- **Embedding size:** $100$.\n",
    "- **Negative samples:** FIX THIS ONE\n",
    "- **Optimizer:** [Nadam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam).\n",
    "- **Early stopping parameters:** We are going to use an Early Stopping criterion on the *validation loss*, with patience $5$ and delta $0.0001$.\n",
    "- **Epochs:** The model will be trained up to $1000$ epochs.\n",
    "- **Learning rate:** since tipically the loss function is quite convex for the embedding problem, we can use a relatively higher learning rate. We are going to us $0.1$ for this example, to get to a faster convergence: this might lead to skipping some better minima that might be identified with a lower learning rate, such as the default one which is $0.0001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length=100\n",
    "batch_size=2**7\n",
    "iterations=20\n",
    "window_size=4\n",
    "p=1.0\n",
    "q=1.0\n",
    "embedding_size=100\n",
    "negatives_samples=7\n",
    "patience=5\n",
    "delta=0.0001\n",
    "epochs=1000\n",
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation Keras sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embiggen import NodeBinarySkipGramSequence\n",
    "\n",
    "training_sequence = NodeBinarySkipGramSequence(\n",
    "    training,\n",
    "    length=walk_length,\n",
    "    batch_size=batch_size,\n",
    "    iterations=iterations,\n",
    "    window_size=window_size,\n",
    "    return_weight=1/p,\n",
    "    explore_weight=1/q\n",
    ")\n",
    "\n",
    "validation_sequence = NodeBinarySkipGramSequence(\n",
    "    graph, # Here we use the entire graph. This will only be used for the early stopping.\n",
    "    length=walk_length,\n",
    "    batch_size=batch_size,\n",
    "    iterations=iterations,\n",
    "    window_size=window_size,\n",
    "    return_weight=1/p,\n",
    "    explore_weight=1/q\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the SkipGram model\n",
    "We are going to setup the model to use, if available, multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BinarySkipGram\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_embedding (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       1718500     words_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       1718500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,437,002\n",
      "Trainable params: 3,437,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from embiggen import BinarySkipGram\n",
    "\n",
    "strategy = MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = BinarySkipGram(\n",
    "        vocabulary_size=training.get_nodes_number(),\n",
    "        embedding_size=embedding_size,\n",
    "        optimizer=Nadam(learning_rate=learning_rate)\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the SkipGram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "135/135 [==============================] - 115s 854ms/step - auroc: 0.7378 - precision: 0.9635 - recall: 0.9957 - auprc: 0.9861 - loss: 0.1610 - val_auroc: 0.7766 - val_precision: 0.9541 - val_recall: 0.9996 - val_auprc: 0.9855 - val_loss: 0.1663\n",
      "Epoch 2/1000\n",
      "135/135 [==============================] - 107s 795ms/step - auroc: 0.8244 - precision: 0.9635 - recall: 0.9995 - auprc: 0.9916 - loss: 0.1313 - val_auroc: 0.7940 - val_precision: 0.9546 - val_recall: 0.9986 - val_auprc: 0.9862 - val_loss: 0.1663\n",
      "Epoch 3/1000\n",
      "135/135 [==============================] - 107s 790ms/step - auroc: 0.7601 - precision: 0.9638 - recall: 0.9857 - auprc: 0.9851 - loss: 0.2687 - val_auroc: 0.6302 - val_precision: 0.9545 - val_recall: 0.9942 - val_auprc: 0.9667 - val_loss: 0.3141\n",
      "Epoch 4/1000\n",
      "135/135 [==============================] - 106s 782ms/step - auroc: 0.7886 - precision: 0.9633 - recall: 0.9997 - auprc: 0.9884 - loss: 0.1390 - val_auroc: 0.7899 - val_precision: 0.9539 - val_recall: 0.9999 - val_auprc: 0.9865 - val_loss: 0.1625\n",
      "Epoch 5/1000\n",
      "135/135 [==============================] - 106s 784ms/step - auroc: 0.8238 - precision: 0.9634 - recall: 0.9998 - auprc: 0.9915 - loss: 0.1306 - val_auroc: 0.7966 - val_precision: 0.9540 - val_recall: 0.9998 - val_auprc: 0.9871 - val_loss: 0.1614\n",
      "Epoch 6/1000\n",
      "135/135 [==============================] - 105s 778ms/step - auroc: 0.8370 - precision: 0.9634 - recall: 0.9998 - auprc: 0.9924 - loss: 0.1280 - val_auroc: 0.7979 - val_precision: 0.9540 - val_recall: 0.9998 - val_auprc: 0.9871 - val_loss: 0.1619\n",
      "Epoch 7/1000\n",
      "135/135 [==============================] - 105s 779ms/step - auroc: 0.8441 - precision: 0.9634 - recall: 0.9997 - auprc: 0.9929 - loss: 0.1265 - val_auroc: 0.7974 - val_precision: 0.9540 - val_recall: 0.9997 - val_auprc: 0.9868 - val_loss: 0.1625\n",
      "Epoch 8/1000\n",
      "135/135 [==============================] - 105s 776ms/step - auroc: 0.8485 - precision: 0.9635 - recall: 0.9997 - auprc: 0.9932 - loss: 0.1254 - val_auroc: 0.7963 - val_precision: 0.9540 - val_recall: 0.9998 - val_auprc: 0.9866 - val_loss: 0.1633\n",
      "Epoch 9/1000\n",
      " 38/135 [=======>......................] - ETA: 58s - auroc: 0.8526 - precision: 0.9638 - recall: 0.9996 - auprc: 0.9935 - loss: 0.1239"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    training_sequence,\n",
    "    steps_per_epoch=training_sequence.steps_per_epoch,\n",
    "    validation_data=validation_sequence,\n",
    "    validation_steps=validation_sequence.steps_per_epoch,\n",
    "    epochs=1000,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            \"val_loss\",\n",
    "            min_delta=delta,\n",
    "            patience=patience,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights\n",
    "We save the obtained model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{model.name}_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training history\n",
    "We can visualize the performance of the model during the training process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some hickups in the plot of the history if the model is reloaded from stored weights: [this is a known Keras issue](https://github.com/keras-team/keras/issues/4875) and is not related to either the holdouts used or the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the obtained embeddings\n",
    "Finally we save our hard earned model embeddings. In another notebook we will show how to do link prediction on the obtained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(f\"{model.name}_embedding.npy\", model.embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
